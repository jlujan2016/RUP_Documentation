<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" lang="es" xml:lang="es">
<head>
<META http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Concepto: Medidas clave de la prueba</title>
<meta content="Concept" name="uma.type">
<meta content="key_measures_of_test" name="uma.name">
<meta content="Medidas clave de la prueba" name="uma.presentationName">
<meta name="element_type" content="concept">
<meta content="description" name="filetype">
<meta name="role" content="">
<link type="text/css" href="./../../../css/default.css" rel="StyleSheet">
<script language="JavaScript" type="text/javascript" src="./../../../scripts/ContentPageResource.js"></script><script language="JavaScript" type="text/javascript" src="./../../../scripts/ContentPageSection.js"></script><script language="JavaScript" type="text/javascript" src="./../../../scripts/ContentPageSubSection.js"></script><script language="JavaScript" type="text/javascript" src="./../../../scripts/ContentPageToolbar.js"></script><script language="JavaScript" type="text/javascript" src="./../../../scripts/contentPage.js"></script><script language="JavaScript" type="text/javascript">
					var backPath = './../../../';
					var imgPath = './../../../images/';
					var nodeInfo=[{view: "view:_FCx1oN7CEdmsEI4YDGX2ag", path: ["_FCx1oN7CEdmsEI4YDGX2ag", "_kC0pcN7GEdm8G6yT7-Wdqw", "_yd3EzdnmEdmO6L4XMImrsA", "5.312818155786224E-305"]}, {view: "view:_FCx1oN7CEdmsEI4YDGX2ag", path: ["_FCx1oN7CEdmsEI4YDGX2ag", "_Jvt1cAIaEdqEutyfYo0quQ", "_EOvXUN7HEdm8G6yT7-Wdqw", "_SPvXcN7IEdm8G6yT7-Wdqw", "{C6AAA2A6-C5AD-42DA-BCBC-9B1349D7B6CF}", "5.312818155786224E-305"]}, {view: "view:_FCx1oN7CEdmsEI4YDGX2ag", path: ["_FCx1oN7CEdmsEI4YDGX2ag", "_Jvt1cAIaEdqEutyfYo0quQ", "_jijhYAIaEdqEutyfYo0quQ", "_Yhh_sCxmEdqYV4MWf8PiCw", "{C6AAA2A6-C5AD-42DA-BCBC-9B1349D7B6CF}", "5.312818155786224E-305"]}, {view: "view:_FCx1oN7CEdmsEI4YDGX2ag", path: ["_FCx1oN7CEdmsEI4YDGX2ag", "_jD8dUAIbEdqEutyfYo0quQ", "_vzRNgDIcEdqDs_9ORT1Rig", "5.312818155786224E-305"]}, {view: "view:_LVCagP5WEdmAzesbYywanQ", path: ["_LVCagP5WEdmAzesbYywanQ", "_mp7z0DIDEdqwaNnSEheSAg", "_u2yEADIEEdqwaNnSEheSAg", "_yd3EzdnmEdmO6L4XMImrsA", "5.312818155786224E-305"]}, {view: "view:_LVCagP5WEdmAzesbYywanQ", path: ["_LVCagP5WEdmAzesbYywanQ", "_mp7z0DIDEdqwaNnSEheSAg", "_4EQgMDIEEdqwaNnSEheSAg", "_SPvXcN7IEdm8G6yT7-Wdqw", "{C6AAA2A6-C5AD-42DA-BCBC-9B1349D7B6CF}", "5.312818155786224E-305"]}];
					contentPage.preload(imgPath, backPath, nodeInfo,  '', false, false, false);
				</script>
</head>
<body>
<div id="breadcrumbs"></div>
<table width="100%" cellspacing="0" cellpadding="0" border="0">
<tr>
<td valign="top"><a name="Top"></a>
<div id="page-guid" value="5.312818155786224E-305"></div>
<table width="100%" cellpadding="0" cellspacing="0" border="0">
<tr>
<td nowrap="true" class="pageTitle">Concepto: Medidas clave de la prueba</td><td width="100%">
<div id="contentPageToolbar" align="right"></div>
</td>
</tr>
</table>
<table cellspacing="0" cellpadding="0" border="0" width="100%">
<tr>
<td class="pageTitleSeparator"><img height="1" title="" alt="" src="./../../../images/shim.gif"></td>
</tr>
</table>
<div class="overview">
<table cellpadding="0" cellspacing="0" border="0" width="97%">
<tr>
<td width="50"><img title="" alt="" src="./../../../images/concept.gif"></td><td>
<table cellpadding="0" cellspacing="0" border="0" class="overviewTable">
<tr>
<td valign="top">En esta directriz se presentan las medidas de calidad y cobertura para los conjuntos de aplicaciones de prueba.</td>
</tr>
</table>
</td>
</tr>
</table>
</div>
<div class="sectionHeading">Relaciones</div>
<div class="sectionContent">
<table cellpadding="0" cellspacing="0" border="0" class="sectionTable">
<tr valign="top">
<th scope="row" class="sectionTableHeading">Elementos relacionados</th><td class="sectionTableCell">
<ul>
<li>
<a href="./../../../core.base_rup/workproducts/rup_test_evaluation_summary_D9B4016B.html" guid="{C6AAA2A6-C5AD-42DA-BCBC-9B1349D7B6CF}">Resumen de evaluación de prueba</a>
</li>
<li>
<a href="./../../../core.base_rup/disciplines/rup_test_discipline_9DFAFB2F.html" guid="_yd3EzdnmEdmO6L4XMImrsA">Prueba</a>
</li>
</ul>
</td>
</tr>
</table>
</div>
<div class="sectionHeading">Descripci&oacute;n principal</div>
<div class="sectionContent">
<table cellpadding="0" cellspacing="0" border="0" class="sectionTable">
<tr valign="top">
<td class="sectionTableSingleCell"><a id="Top" name="Top"></a><a key="prueba" text="medidas clave de" name="XE_test__key_measures_of" id="XE_test__key_measures_of" class="index"></a><a key="medidas" text="medidas clave de la prueba" name="XE_measures__key_measures_of_test" id="XE_measures__key_measures_of_test" class="index"></a> 
<h3>
    <b><a id="Introduction" name="Introduction">Introducción</a></b>
</h3>
<p>
    Las medidas clave de una prueba incluyen la cobertura y la calidad.
</p>
<p>
    La cobertura de la prueba es la medida de la completitud de la prueba y se basa en la cobertura de la prueba expresada
    mediante la cobertura de los requisitos de prueba y los guiones de prueba o mediante la cobertura del código ejecutado.
</p>
<p>
    La calidad es una medida de fiabilidad, estabilidad y rendimiento del destino de la prueba (sistema o aplicación que se
    está probando). La calidad se basa en la evaluación de los resultados de la prueba y el análisis de las solicitudes de
    cambio (defectos) identificados durante la prueba.
</p>
<h3>
    <a key="medidas" text="tipos de cobertura de la prueba" name="XE_measures__types_of_test_coverage" id="XE_measures__types_of_test_coverage" class="index"></a><b><a id="Coverage" name="Coverage">Medidas de
    cobertura</a></b>
</h3>
<p>
    La métrica de cobertura responde a la pregunta: "¿Qué porcentaje de la prueba está completo?" Las medidas de cobertura
    que se utilizan con más frecuencia se basan en la cobertura de los requisitos de software y el código fuente.
    Básicamente, la cobertura de la prueba es cualquier medida de completitud con respecto a un requisito (basada en
    requisitos) o los criterios de implementación y diseño de código (basada en código), como la verificación de guiones de
    uso (basada en requisitos) o la ejecución de todas las líneas de código (basada en código).
</p>
<p>
    Cualquier tarea de prueba sistemática se basa en, como mínimo, una estrategia de cobertura de la prueba. La estrategia
    de cobertura guía el diseño de los guiones de prueba indicando el objetivo general de la prueba. La sentencia de la
    estrategia de cobertura puede ser tan simple como verificar todo el rendimiento.
</p>
<p>
    Una estrategia de cobertura basada en requisitos puede ser suficiente para producir una medida cuantificable de la
    completitud de la prueba si los requisitos se catalogan completamente. Por ejemplo, si se han identificado todos los
    requisitos de la prueba de rendimiento, los resultados de la prueba se pueden consultar para obtener medidas; por
    ejemplo, se ha verificado el 75% de los requisitos de la prueba de rendimiento.
</p>
<p>
    Si se aplica la cobertura basada en código, las estrategias de prueba se formulan en términos de qué cantidad del
    código fuente han ejecutado las pruebas. Este tipo de estrategia de cobertura de la prueba es muy importante para
    sistemas de seguridad crítica.
</p>
<p>
    Las dos medidas se pueden derivar manualmente (utilizando las ecuaciones que se proporcionan en las dos cabeceras
    siguientes) o se pueden calcular con las herramientas de automatización de pruebas.
</p>
<h4>
    <a id="Requirements-based test coverage" name="Requirements-based test coverage">Cobertura de la prueba basada en
    requisitos</a>
</h4>
<p>
    La cobertura de la prueba basada en requisitos, medida varias veces durante el ciclo vital de la prueba, identifica la
    cobertura de la prueba en un hito del ciclo vital de pruebas, como la cobertura de la prueba satisfactoria, ejecutada,
    implementada y planificada.
</p>
<ul>
    <li>
        La cobertura de la prueba se calcula con la siguiente ecuación:
    </li>
</ul>
<blockquote>
    <p class="example">
        Cobertura de la prueba = T<sup><sup>(p,i,x,s)</sup></sup> / RfT<br />
        <br />
         Donde:<br />
         T es el número de pruebas (planificadas, implementadas, ejecutadas o satisfactorias), expresadas como
        procedimientos de prueba o guiones de prueba.
    </p>
    <p class="example">
        RfT es el número total de requisitos de la prueba.
    </p>
</blockquote>
<ul>
    <li>
        En la tarea Planificar prueba, la cobertura de la prueba se calcula para determinar la cobertura de la prueba
        planificada de la siguiente manera:
    </li>
</ul>
<blockquote>
    <p class="example">
        Cobertura de la prueba (planificada) = T<sup><sup>p</sup></sup> / RfT<br />
        <br />
         Donde:<br />
         T<sup><sup>p</sup></sup> es el número de pruebas planificadas, expresadas como procedimientos de pruebas o guiones
        de prueba.
    </p>
    <p class="example">
        RfT es el número total de requisitos de la prueba.
    </p>
</blockquote>
<ul>
    <li>
        En la tarea Implementar prueba, cuando se implementan los procedimientos de prueba (como scripts de prueba), se
        calcula la cobertura de la prueba con la siguiente ecuación:
    </li>
</ul>
<blockquote>
    <p class="example">
        Cobertura de la prueba (implementada) = T<sup><sup>i</sup></sup> / RfT<br />
        <br />
         aquí:<br />
         T<sup><sup>i</sup></sup> es el número de pruebas implementadas, expresado mediante el número de procedimientos de
        prueba o guiones de prueba para los que hay scripts de prueba correspondientes.
    </p>
    <p class="example">
        RfT es el número total de requisitos de la prueba.
    </p>
</blockquote>
<ul>
    <li>
        En la tarea Ejecutar prueba, se utilizan dos medidas de cobertura de la prueba: una identifica la cobertura de la
        prueba que se consigue mediante la ejecución de pruebas y la segunda identifica la cobertura de la prueba
        satisfactoria (las pruebas que se ejecutaron sin anomalías, como defectos o resultados inesperados). 
        <p>
            Estas medidas de cobertura se calculan con las siguientes ecuaciones:
        </p>
        <blockquote>
            <p class="example">
                Cobertura de la prueba (ejecutada) = T<sup><sup>x</sup></sup> / RfT
            </p>
        </blockquote>
        <blockquote>
            <p class="example">
                Donde:<br />
                 T<sup><sup>x</sup></sup> es el número de pruebas ejecutadas, expresadas como procedimientos de prueba o
                guiones de prueba.
            </p>
        </blockquote>
        <blockquote>
            <p class="example">
                RfT es el número total de requisitos de la prueba.
            </p>
        </blockquote>
    </li>
    <li style="LIST-STYLE-TYPE: none">
        <br />
        <br />
    </li>
</ul>
<blockquote>
    <p class="example">
        Cobertura de la prueba satisfactoria (ejecutada) = T<sup><sup>s</sup></sup> / RfT
    </p>
    <blockquote>
        <p class="example">
            Donde:<br />
             T<sup><sup>s</sup></sup> es el número de pruebas ejecutadas, expresadas como procedimientos de prueba o
            guiones de prueba que se completaron satisfactoriamente, sin defectos.
        </p>
    </blockquote>
    <blockquote>
        <p class="example">
            RfT es el número total de requisitos de la prueba.
        </p>
    </blockquote>
</blockquote>
<div style="MARGIN-LEFT: 4em">
    <br />
    <br />
</div>
<p>
    Si convertimos las proporciones anteriores en porcentajes, se admite la siguiente sentencia de cobertura de la prueba
    basada en requisitos:
</p>
<blockquote>
    <p>
        El x% de los guiones de prueba (T<sup><sup>(p,i,x,s)</sup></sup> en las ecuaciones anteriores) se ha cubierto con
        un índice de satisfacción del y%
    </p>
</blockquote>
<p>
    Esta sentencia significativa de la cobertura de la prueba se puede comparar con los criterios de satisfacción
    definidos. Si no se han cumplido los criterios, la sentencia proporciona una base para predecir la cantidad de esfuerzo
    de prueba que resta.
</p>
<h4>
    <a id="Code-based test coverage" name="Code-based test coverage">Cobertura de la prueba basada en código</a>
</h4>
<p>
    La cobertura de la prueba basada en código mide la cantidad de código que se ejecutó durante la prueba, en comparación
    con la cantidad de código que queda por ejecutar. La cobertura de código puede basarse en flujos de control (sentencia,
    ramificación o vías de acceso) o flujos de datos.
</p>
<ul>
    <li>
        En la cobertura del flujo de control, el objetivo es probar las líneas de código, las condiciones de ramificación,
        las vías de acceso del código u otros elementos del flujo de control del software.
    </li>
    <li>
        En la cobertura del flujo de datos, el objetivo es comprobar que el estado de los datos es válido durante el
        funcionamiento del software; por ejemplo, que un elemento de datos se define antes de utilizarse.
    </li>
</ul>
<p>
    La cobertura de la prueba basada en código se calcula con la siguiente ecuación:
</p>
<blockquote>
    <p class="example">
        Cobertura de la prueba = I<sup><sup>e</sup></sup> / TIic
    </p>
</blockquote>
<blockquote>
    <p class="example">
        Donde:<br />
         I<sup><sup>e</sup></sup> es el número de elementos ejecutados, expresados como sentencias de código,
        ramificaciones de código, vías de acceso de código, puntos de decisión del estado de los datos o nombres de
        elementos de datos.
    </p>
</blockquote>
<blockquote>
    <p class="example">
        TIic es el número total de elementos del código.
    </p>
</blockquote><br />
<br />
<p>
    Si se convierte esta proporción en un porcentaje, se admite la siguiente sentencia de cobertura de la prueba basada en
    código:
</p>
<blockquote>
    <p>
        El x% de los guiones de prueba (I en la ecuación anterior) se han cubierto con un índice de satisfacción del y%
    </p>
</blockquote>
<p>
    Esta sentencia significativa de la cobertura de la prueba se puede comparar con los criterios de satisfacción
    definidos. Si no se han cumplido los criterios, la sentencia proporciona una base para predecir la cantidad de esfuerzo
    de prueba que resta.
</p>
<h3>
    <a key="medidas" text="para calidad percibida" name="XE_measures__for_perceived_quality" id="XE_measures__for_perceived_quality" class="index"></a><b><a id="Quality" name="Quality">Medida de la calidad
    percibida</a></b>
</h3>
<p>
    Aunque la evaluación de la cobertura de la prueba proporciona una medida del grado de completitud del esfuerzo de
    prueba, la evaluación de los defectos descubiertos durante la prueba proporciona la mejor indicación de la calidad del
    software tal como se ha experimentado. Esta percepción de calidad se puede utilizar para razonar sobre la calidad
    general del sistema de software en su conjunto. La calidad del software percibida es una medida del grado en que el
    software cumple los requisitos que se le imponen; por lo tanto, en este contexto, los defectos se consideran como un
    tipo de solicitud de cambio en el que el destino de la prueba no cumplió los requisitos de software.
</p>
<p>
    La evaluación de defectos puede basarse en métodos que van desde simples recuentos de defectos hasta un riguroso
    modelado estadístico.
</p>
<p>
    La evaluación rigurosa utiliza conjeturas sobre la velocidad de descubrimiento o llegada de defectos durante el proceso
    de prueba. Un modelo común presupone que la velocidad está de acuerdo con la distribución de Poisson. Los datos reales
    sobre la velocidad de los defectos se ajustan al modelo. La evaluación resultante calcula la fiabilidad actual del
    software y predice cómo aumentará la fiabilidad si continúan la eliminación de defectos y las pruebas. Esta evaluación
    se describe como modelado del crecimiento de la fiabilidad del software y es un área de estudio activo. Debido a la
    falta de soporte de herramientas para este tipo de evaluación, es recomendable equilibrar cuidadosamente el coste de
    utilizar este enfoque con los beneficios obtenidos.
</p>
<p>
    El <b>análisis de defectos</b> incluye el análisis de la distribución de defectos a los valores de uno o varios de los
    atributos asociados a un defecto. El análisis de defectos proporciona una indicación de la fiabilidad del software.
</p>
<p>
    En el análisis de defectos, normalmente se analizan cuatro atributos de defectos principales:
</p>
<ul>
    <li>
        <b>Estado</b>: el estado actual del defecto (abierto, arreglándose, cerrado, etc).
    </li>
    <li>
        <b>Prioridad</b>: la importancia relativa de este defecto que se está resolviendo.
    </li>
    <li>
        <b>Gravedad</b>: el impacto relativo de este defecto para el usuario, una organización, terceros, etc.
    </li>
    <li>
        <b>Origen</b>: dónde está y cuál es el error de origen que provoca este defecto o qué componente se va a arreglar
        para eliminar este defecto.
    </li>
</ul>
<p>
    Los recuentos de defectos se pueden notificar como una función de tiempo, creando un informe o diagrama de la tendencia
    de defectos. También se pueden notificar en un informe de densidad de defectos como una función de uno o varios
    atributos de defectos, como la gravedad o el estado. Estos tipos de análisis proporcionan una perspectiva de las
    tendencias o de la distribución de defectos que revela la fiabilidad del software.<br />
    <br />
     Por ejemplo, es de esperar que la velocidad de descubrimiento de defectos disminuya a medida que avanzan las pruebas y
    los arreglos. Se puede establecer un umbral de defectos o calidad pobre que indique el punto en que la calidad del
    software pasa a ser inaceptable. Los recuentos de defectos también se pueden notificar en función del origen del modelo
    de implementación, lo que permite detectar "módulos débiles", "zonas activas" y componentes de software que se arreglan
    una y otra vez, lo que indica errores de diseño más fundamentales.
</p>
<p>
    En un análisis de este tipo sólo se incluyen defectos confirmados. No todos los defectos notificados denotan un error
    real; algunos son solicitudes de mejora fuera del ámbito del proyecto o describen un defecto que ya se ha notificado.
    Sin embargo, merece la pena analizar por qué se notifican muchos defectos que son duplicados o defectos no confirmados.
</p>
<h4>
    <a id="Defect Reports" name="Defect Reports">Informes de defectos</a>
</h4>
<p>
    Rational Unified Process recomienda la evaluación de defectos basada en varias categorías de informes, como se muestra
    a continuación:
</p>
<ul>
    <li>
        Los informes de distribución (densidad) de defectos permiten que se muestren los recuentos de defectos como una
        función de uno o dos atributos de defectos.
    </li>
    <li>
        Los informes de antigüedad de defectos son un tipo especial de informe de distribución de defectos. Los informes de
        antigüedad de defectos muestran el tiempo que lleva un defecto en un estado en particular, por ejemplo, Abierto. En
        cualquier categoría de antigüedad, los defectos también se pueden ordenar según otro atributo, por ejemplo,
        Propietario.
    </li>
    <li>
        Los informes de tendencia de defectos muestran los recuentos de defectos, por estado (nuevo, abierto o cerrado),
        como una función de tiempo. Los informes de tendencia pueden ser acumulativos o no acumulativos.
    </li>
</ul>
<p>
    Muchos de estos informes son valiosos para evaluar la calidad del software. Son especialmente útiles cuando se analizan
    junto con los resultados de la prueba y los informes de progreso que muestran los resultados de las pruebas que se
    realizaron en una serie de iteraciones y ciclos de prueba para la aplicación que se está probando. Los criterios de
    prueba habituales incluyen una sentencia sobre el número tolerable de defectos abiertos en determinadas categorías,
    como la clase de gravedad, que se comprueba fácilmente con una evaluación de la distribución de defectos. Al clasificar
    y agrupar esta distribución según motivadores de prueba, la evaluación se puede centrar en áreas de preocupación
    importantes.
</p>
<p>
    Normalmente, es necesario el soporte de herramientas para crear eficazmente informes de este tipo.
</p>
<h4>
    <b><a id="Defect density reports:" name="Defect density reports:">Informes de la densidad de defectos</a></b>
</h4>
<h5>
    <b>Prioridad en oposición a estado del defecto</b>
</h5>
<p>
    Proporcione a cada defecto una prioridad. Normalmente, resulta práctico y es suficiente tener cuatro niveles de
    prioridad, por ejemplo:
</p>
<ul>
    <li>
        Prioridad urgente (resolver inmediatamente)
    </li>
    <li>
        Prioridad alta
    </li>
    <li>
        Prioridad normal
    </li>
    <li>
        Prioridad baja
    </li>
</ul>
<p>
    <b>Nota</b>: los criterios para una prueba satisfactoria se pueden expresar en términos del aspecto que debería tener
    la distribución de defectos en estos niveles de prioridad. Por ejemplo, los criterios de prueba satisfactorios pueden
    ser "están abiertos cero defectos de Prioridad 1 y menos de cinco defectos de Prioridad 2". Debe generarse un diagrama
    de distribución de defectos, como el siguiente.
</p>
<p align="center">
    <img height="233" alt="Diagrama de distribución de defectos" src="./../../../core.base_rup/guidances/concepts/resources/keymeas1.gif" width="378" />
</p><br />
<br />
<p>
    Está claro que no se han satisfecho los criterios. Este diagrama debe incluir un filtro para mostrar únicamente los
    defectos abiertos, como requieren los criterios de la prueba.
</p>
<h5>
    <b>Gravedad en oposición a estado del defecto</b>
</h5>
<p>
    Los informes de gravedad de defectos muestran cuántos defectos hay en cada clase de gravedad; por ejemplo, error muy
    grave, función principal no realizada, error leve.
</p>
<h5>
    <b>Ubicación es oposición a estado del defecto en el modelo de implementación</b>
</h5>
<p>
    Los informes de origen de defectos muestran la distribución de los defectos en los elementos del modelo de
    implementación.
</p>
<h4>
    <b><a id="Defect aging reports:" name="Defect aging reports:">Informes de antigüedad de defectos</a></b>
</h4>
<p>
    El análisis de la antigüedad de defectos proporciona información fiable sobre la eficacia de las pruebas y las tareas
    de eliminación de defectos. Por ejemplo, si la mayoría de los defectos antiguos, sin resolver, se encuentran en estado
    de validación pendiente, probablemente significa que no se han aplicado los suficientes recursos al esfuerzo de repetir
    la prueba.
</p>
<h4>
    <b><a id="Defect trend reports:" name="Defect trend reports:">Informes de tendencia de defectos</a></b>
</h4>
<p>
    Los informes de tendencia de defectos identifican los índices de defectos y proporcionan una vista especialmente buena
    del estado de la prueba. Las tendencias de defectos siguen un patrón bastante previsible en un ciclo de prueba. Al
    principio del ciclo, los índices de defectos aumentan con rapidez, hasta que alcanzan un pico y empiezan a disminuir a
    una velocidad menor a lo largo del tiempo.
</p><br />
<br />
<p align="center">
    <img height="230" alt="Diagrama de informes de tendencia de defectos" src="./../../../core.base_rup/guidances/concepts/resources/keymeas2.gif" width="373" />
</p>
<p>
    Para buscar problemas, la planificación del proyecto se puede revisar teniendo en cuenta esta tendencia. Por ejemplo,
    si los índices de defectos siguen aumentando en la tercera semana de un ciclo de prueba de cuatro semanas, no cabe duda
    de que el proyecto no está siguiendo la planificación.
</p>
<p>
    Este sencillo análisis de tendencias presupone que los defectos se están arreglando puntualmente y que los arreglos se
    están probando en compilaciones posteriores, de modo que la velocidad de cerrar defectos debería tener el mismo perfil
    que la velocidad de encontrar defectos. Cuando esto no sucede, indica que hay un problema con el proceso de resolución
    de defectos; los recursos de arreglo de defectos o los recursos para repetir la prueba y validar los arreglos podrían
    ser inapropiados.
</p>
<p align="center">
    <img height="230" alt="Diagrama de informes de análisis de tendencias" src="./../../../core.base_rup/guidances/concepts/resources/keymeas3.gif" width="469" />
</p>
<p>
    La tendencia que se refleja en este informe muestra que se han descubierto defectos nuevos y se han abierto rápidamente
    al principio del proyecto, y que disminuyen con el tiempo. La tendencia para abrir defectos es similar a la tendencia
    para defectos nuevos, aunque va un poco por detrás. La tendencia para cerrar defectos aumenta con el tiempo, a medida
    que se arreglan y verifican los efectos abiertos. Estas tendencias representan un esfuerzo satisfactorio.
</p>
<p>
    Si sus tendencias se desvían dramáticamente de estas, pueden indicar un problema e identificar cuándo es necesario
    aplicar recursos adicionales a áreas específicas de desarrollo o prueba.
</p>
<p>
    Cuando se combinan con las medidas de cobertura de la prueba, el análisis de defectos proporciona una evaluación
    excelente para basar los criterios de terminación.
</p>
<h3>
    <a id="Performance" name="Performance">Medidas de rendimiento</a>
</h3>
<p>
    Se utilizan varias medidas para evaluar los comportamientos de rendimiento del destino de la prueba y para centrarse en
    la captura de datos relacionados con los comportamientos, como el tiempo de respuesta, los perfiles de tiempo, el flujo
    de ejecución, la fiabilidad operativa y los límites. Principalmente, estas medidas se evalúan en la tarea Evaluar
    prueba; sin embargo, hay medidas de rendimiento que se utilizan durante la tarea Ejecutar prueba para evaluar el estado
    y el progreso de la prueba.
</p>
<p>
    Las principales medidas de rendimiento son:
</p>
<ul>
    <li>
        <b>Supervisión dinámica</b>: captura y visualización en tiempo real del estado de cada script de prueba que se
        ejecuta durante la ejecución de la prueba.
    </li>
    <li>
        <b>Informes de rendimiento y tiempo de respuesta</b>: medida de los tiempos de respuesta y el rendimiento del
        destino de la prueba para los actores y guiones de uso especificados.
    </li>
    <li>
        <b>Informes de percentil</b>: medida y cálculo del percentil de los valores recopilados de los datos.
    </li>
    <li>
        <b>Informes de comparación</b>: diferencias o tendencias entre dos (o más) conjuntos de datos que representan
        diferentes ejecuciones de la prueba.
    </li>
    <li>
        <b>Informes de rastreo</b>: detalles de los mensajes y conversaciones entre el actor (script de prueba) y el
        destino de la prueba.
    </li>
</ul>
<h4>
    <a id="Dynamic Monitoring" name="Dynamic Monitoring">Supervisión dinámica</a>
</h4>
<p>
    La supervisión dinámica proporciona visualización e informes en tiempo real durante la ejecución de la prueba,
    normalmente en formato de histograma o gráfico. El informe supervisa o evalúa la ejecución de pruebas de rendimiento
    mediante la visualización del estado actual y el progreso de los scripts de prueba.
</p>
<p align="center">
    <img height="333" alt="Supervisión dinámica visualizada como un histograma" src="./../../../core.base_rup/guidances/concepts/resources/keymeas4.gif" width="501" />
</p>
<p>
    Por ejemplo, en el histograma precedente, hay 80 scripts de prueba ejecutando el mismo guión de uso. En este gráfico,
    hay 14 scripts de prueba en estado Desocupado, 12 en estado Consulta, 34 en estado Ejecución de SQL, 4 en estado de
    Conexión SQL y 16 en estado Otro. A medida que progresa la prueba, verá cómo cambia el número de scripts en cada
    estado. La salida visualizada será típica de la ejecución de una prueba que progresa con normalidad y se encuentra en
    medio de la ejecución. Sin embargo, si los scripts de prueba permanecen en un estado o no muestran cambios durante la
    ejecución de la prueba, podría ser una indicación de que existe un problema en la ejecución de la prueba, o la
    necesidad de implementar o evaluar otras medidas de rendimiento.
</p>
<h4>
    <a id="Response time and throughput reports" name="Response time and throughput reports">Informes de rendimiento y
    tiempo de respuesta</a>
</h4>
<p>
    Los informes de rendimiento y tiempo de respuesta, como su nombre indica, miden y calculan los comportamientos de
    rendimiento relacionados con el tiempo y el rendimiento (número de transacciones procesadas). Normalmente, estos
    informes se muestran como un gráfico con tiempo de respuesta (o número de transacciones) en el eje "y" y sucesos en el
    eje "x".
</p>
<p align="center">
    <img height="333" alt="Diagrama de informes de análisis y rendimiento de ejemplo" src="./../../../core.base_rup/guidances/concepts/resources/keymeas5.gif"     width="499" />
</p>
<p>
    Normalmente, es recomendable calcular y visualizar la información estadística, como la desviación media y estándar de
    los valores de datos, además de mostrar los comportamientos de rendimiento real.
</p>
<h4>
    <a id="Percentile Reports" name="Percentile Reports">Informes de percentil</a>
</h4>
<p>
    Los informes de percentil proporcionan otro cálculo estadístico de rendimiento mediante la muestra de valores de
    percentil de la población para los tipos de datos recopilados.
</p>
<p align="center">
    <img height="333" alt="Diagrama de informes de percentil de ejemplo" src="./../../../core.base_rup/guidances/concepts/resources/keymeas6.gif" width="499" />
</p>
<h4>
    <a id="Comparison Reports" name="Comparison Reports">Informes de comparación</a>
</h4>
<p>
    Es importante comparar los resultados de la ejecución de una prueba de rendimiento con los de otra, para poder evaluar
    el impacto de los cambios realizados entre las ejecuciones de la prueba en los comportamientos de rendimiento. Utilice
    los informes de comparación para visualizar la diferencia entre dos conjuntos de datos (cada uno representa una
    ejecución de prueba diferente) o tendencias entre muchas ejecuciones de prueba.
</p>
<h4>
    <a id="Trace and Profile Reports" name="Trace and Profile Reports">Informes de rastreo y perfil</a>
</h4>
<p>
    Cuando los comportamientos de rendimiento son inaceptables o la supervisión de rendimiento indica posibles cuellos de
    botella (por ejemplo, scripts de prueba que permanecen en un estado dado durante periodos de tiempo muy largos), el
    informe de rastreo podría ser el informe más valioso. Los informes de rastreo y perfil muestran información de bajo
    nivel. Esta información incluye los mensajes entre el actor y el destino de la prueba, el flujo de ejecución, el acceso
    de datos y las llamadas al sistema y a funciones.
</p><br />
<br /></td>
</tr>
</table>
</div>
<table cellpadding="0" cellspacing="0" border="0" class="copyright">
<tr>
<td class="copyright"><p>
    &copy; &nbsp;Copyright IBM Corp.&nbsp;1987, 2006. &nbsp;Reservados todos los derechos.
</p></td>
</tr>
</table>
</td>
</tr>
</table>
</body>
<script language="JavaScript" type="text/javascript">
				contentPage.onload();
			</script>
</html>
